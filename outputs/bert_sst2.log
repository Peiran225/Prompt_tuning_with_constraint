Namespace(similarity=None, log_file=None, path='bert-base-uncased', hook_layer=-1, prompts_dir='/fs/nexus-scratch/peiran/prompting_with_constraints/prompts', prompt_groups=['T', 'R', 'U', 'E'], prompt='SST-2_4', task='SST-2', pile_len=-1, learning_rate=0.001, learning_rate_LM=0.001, gamma=5e-07, epoch=1, seed=47, num_of_initial_text=10000, particular_layer=-3, baseline_only=False, base_initial='Text')
finishing tokeninzing
baseline_only
False
train the model when the hook layer is -2
trainable params: 12,290 || all params: 109,496,068 || trainable%: 0.01122414733650527
{'loss': 0.5006, 'learning_rate': 0.0007624703087885985, 'epoch': 0.24}
{'loss': 0.3161, 'learning_rate': 0.0005249406175771972, 'epoch': 0.48}
{'loss': 0.3125, 'learning_rate': 0.00028741092636579575, 'epoch': 0.71}
{'loss': 0.2879, 'learning_rate': 4.98812351543943e-05, 'epoch': 0.95}
{'eval_loss': 0.30057018995285034, 'eval_accuracy': 0.8830275229357798, 'eval_runtime': 1.109, 'eval_samples_per_second': 786.3, 'eval_steps_per_second': 25.248, 'epoch': 1.0}
{'train_runtime': 201.3448, 'train_samples_per_second': 334.496, 'train_steps_per_second': 10.455, 'train_loss': 0.3516130886848069, 'epoch': 1.0}
trainer.metrics_log [{'epoch': 1.0, 'eval_loss': 0.30057018995285034, 'eval_accuracy': 0.8830275229357798}]
